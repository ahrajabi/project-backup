{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'start', 'to', 'ponder', 'about', 'how', 'might', 'we', 'derive', 'meaning', 'by', 'looking', 'at', 'these', 'words', '.']\n",
      "خانه‌ی\n",
      "پدری\n",
      "کتاب‌ها\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from __future__ import unicode_literals\n",
    "import hazm as hm\n",
    "\n",
    "FARSI_TEXT ='''\n",
    "خانه ی پدری\n",
    "کتاب ها\n",
    "'''\n",
    "\n",
    "EXAMPLE_TEXT = \" We start to ponder about how might we derive meaning by looking at these words.\"\n",
    "norm = hm.Normalizer()\n",
    "FARSI_TEXT = norm.normalize(FARSI_TEXT)\n",
    "print(word_tokenize(EXAMPLE_TEXT))\n",
    "\n",
    "for i in hm.word_tokenize(FARSI_TEXT):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('../dataset/stopwords.csv' , )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ولی', 'برای', 'پردازش', '،', 'جدا', 'بهتر', 'نیست', '؟']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "normalizer = Normalizer()\n",
    "normalizer.normalize('اصلاح نويسه ها و استفاده از نیم‌فاصله پردازش را آسان مي كند')\n",
    "\n",
    "sent_tokenize('ما هم برای وصل کردن آمدیم! ولی برای پردازش، جدا بهتر نیست؟')\n",
    "word_tokenize('ولی برای پردازش، جدا بهتر نیست؟')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'رفت#رو'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "stemmer.stem('کتاب‌ها')\n",
    "lemmatizer = Lemmatizer()\n",
    "lemmatizer.lemmatize('می‌روم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = POSTagger(model='resources/postagger.model')\n",
    "tagger.tag(word_tokenize('ما بسیار کتاب می‌خوانیم'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chunker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-58c1f5cfc3cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'resources/chunker.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'کتاب خواندن را دوست داریم'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtree2brackets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Chunker' is not defined"
     ]
    }
   ],
   "source": [
    "chunker = Chunker(model='resources/chunker.model')\n",
    "tagged = tagger.tag(word_tokenize('کتاب خواندن را دوست داریم'))\n",
    "tree2brackets(chunker.parse(tagged))\n",
    "\n",
    "parser = DependencyParser(tagger=tagger, lemmatizer=lemmatizer)\n",
    "parser.parse(word_tokenize('زنگ‌ها برای که به صدا درمی‌آید؟'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['خوردم', 'خوردی', 'خورد', 'خوردیم', 'خوردید', 'خوردند', 'نخوردم', 'نخوردی', 'نخورد', 'نخوردیم', 'نخوردید', 'نخوردند', 'خورم', 'خوری', 'خورد', 'خوریم', 'خورید', 'خورند', 'نخورم', 'نخوری', 'نخورد', 'نخوریم', 'نخورید', 'نخورند', 'می\\u200cخوردم', 'می\\u200cخوردی', 'می\\u200cخورد', 'می\\u200cخوردیم', 'می\\u200cخوردید', 'می\\u200cخوردند', 'نمی\\u200cخوردم', 'نمی\\u200cخوردی', 'نمی\\u200cخورد', 'نمی\\u200cخوردیم', 'نمی\\u200cخوردید', 'نمی\\u200cخوردند', 'خورده\\u200cام', 'خورده\\u200cای', 'خورده', 'خورده\\u200cایم', 'خورده\\u200cاید', 'خورده\\u200cاند', 'نخورده\\u200cام', 'نخورده\\u200cای', 'نخورده', 'نخورده\\u200cایم', 'نخورده\\u200cاید', 'نخورده\\u200cاند', 'خورم', 'خوری', 'خورد', 'خوریم', 'خورید', 'خورند', 'نخورم', 'نخوری', 'نخورد', 'نخوریم', 'نخورید', 'نخورند', 'می\\u200cخورم', 'می\\u200cخوری', 'می\\u200cخورد', 'می\\u200cخوریم', 'می\\u200cخورید', 'می\\u200cخورند', 'نمی\\u200cخورم', 'نمی\\u200cخوری', 'نمی\\u200cخورد', 'نمی\\u200cخوریم', 'نمی\\u200cخورید', 'نمی\\u200cخورند', 'بخورم', 'بخوری', 'بخورد', 'بخوریم', 'بخورید', 'بخورند', 'نخورم', 'نخوری', 'نخورد', 'نخوریم', 'نخورید', 'نخورند', 'بخور', 'نخور']\n"
     ]
    }
   ],
   "source": [
    "s = lemmatizer.conjugations(\"خورد#خور\")\n",
    "print(s,sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.061989068984985 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from hazm import *\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "normalizer = Normalizer()\n",
    "inputs = normalizer.normalize(inputs)\n",
    "\n",
    "tagger = POSTagger(model='../dataset/tagger/postagger.model')\n",
    "chunker = Chunker(model='../dataset/tagger/chunker.model')\n",
    "\n",
    "print(\"--- %s seconds ---\" % ( time.time() - start_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('روز', 'Ne'), ('دوشنبه', 'N'), ('امتحان', 'N'), ('دارم', 'V'), ('.', 'PUNC'), ('این', 'DET'), ('کتاب', 'N'), ('زرد', 'AJ'), ('است', 'V'), ('.', 'PUNC')]\n",
      "[روز دوشنبه ADVP] [امتحان دارم VP] . [این کتاب NP] [زرد ADJP] [است VP] .\n",
      "--- 0.007989883422851562 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "inputs = '''\n",
    "پیکره متنی زبان فارسی، مجموعه‌ای از متون نوشتاری و گفتاری زبان فارسی به صورت رسمی است که از منابع واقعی همچون روزنامه‌ها، سایت‌ها و مستنداتِ از قبل تایپ شده، جمع‌آوری شده، تصحیح گردیده و برچسب خورده است. حجم این دادگان حدوداً ۱۰۰ میلیون کلمه است و از منابع مختلف تهیه گردیده و دارای تنوعات بسیار زیادی است. ۱۰ میلیون کلمه از این پیکره با استفاده از ۸۸۲ برچسب نحوی-معنایی به صورت دستی توسط دانشجویان رشته زبان‌شناسی برچسب‌دهی شده‌اند و هر پرونده بر حسب موضوع و منبع آن طبقه‌بندی شده است. این پیکره که توسط پژوهشکده پردازش هوشمند علائم تهیه شده است، برای استفاده در تعلیم مدل زبانی و سایر پروژه‌های مربوط به پردازش زبان طبیعی مناسب است.\n",
    "من دوشنبه به خانه می‌روم.\n",
    "'''\n",
    "\n",
    "inputs = '''\n",
    "روز دوشنبه امتحان دارم.\n",
    "این کتاب زرد است.\n",
    "'''\n",
    "\n",
    "inputs = normalizer.normalize(inputs)\n",
    "tagged = tagger.tag(word_tokenize(inputs))\n",
    "print(tagged)\n",
    "print(tree2brackets(chunker.parse(tagged)))\n",
    "print(\"--- %s seconds ---\" % ( time.time() - start_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
