{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import redis\n",
    "import time\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'redis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8e4389b6289b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrictRedis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mswfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dataset/stopwords.csv'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'redis' is not defined"
     ]
    }
   ],
   "source": [
    "r = redis.StrictRedis()\n",
    "swfa = pd.read_csv('../dataset/stopwords.csv' , header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82120000\n",
      "82120500\n",
      "82121000\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(82120000,82154440):#range(82120000,82154440):\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "    if r.exists('irna#'+str(i)):\n",
    "        continue\n",
    "    url = 'http://www.irna.ir/fa/News/'+str(i)+'/'\n",
    "    page = requests.get(url).text\n",
    "    soup = bs(page , \"html.parser\")\n",
    "    if soup :\n",
    "        body = soup.find('div',class_='BodyText')\n",
    "        if body:\n",
    "            body = body.find('p')\n",
    "            r.set('irna#'+str(i) , body.text)\n",
    "        else:\n",
    "            r.set('irna#'+str(i) , \"None\")\n",
    "    else:\n",
    "        r.set('irna#'+str(i) , \"None\")\n",
    "print(\"--- %s seconds ---\" % ( time.time() - start_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "--- 63.86656975746155 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from hazm import *\n",
    "tagger = POSTagger(model='../dataset/tagger/postagger.model')\n",
    "#tagger = StanfordPOSTagger(model_filename='../dataset/persian.tagger', path_to_jar='../dataset/stanford-postagger.jar')\n",
    "clean_train = []\n",
    "clean_train_original = []\n",
    "cnt = 0\n",
    "for i in r.keys(pattern='irna#82120[0|1|2]**'):\n",
    "    body = r.get(i)\n",
    "    cnt += 1\n",
    "    if cnt % 50 == 0:\n",
    "        print(cnt)\n",
    "    if body == 'None':\n",
    "        continue\n",
    "    body = body.decode('utf-8')\n",
    "#    print(body)\n",
    "    norm = Normalizer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    body = norm.normalize(body)\n",
    "    words = word_tokenize(body)\n",
    "    words = [word for word in words if word not in list(swfa[0]) ]\n",
    "    for idx , j in enumerate(words):\n",
    "#        words[idx] = stemmer.stem(j)\n",
    "        k = tagger.tag([words[idx]])\n",
    "#        print(k)\n",
    "        if k[0][1] in [\"PUNC\",\"NUM\",\"V\",\"ADV\",\"AJ\"] :\n",
    "            words[idx] = ''\n",
    "            continue\n",
    "        words[idx] = lemmatizer.lemmatize(words[idx] , pos =k[0][1])\n",
    "\n",
    "        #print(tagger.tag(j))\n",
    "    clean_train_original.append(words)\n",
    "    clean_train.append( \" \".join( words ) )\n",
    "#    print(clean_train)\n",
    "\n",
    "print(\"--- %s seconds ---\" % ( time.time() - start_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 1000)\n",
    "train_data_features_original = vectorizer.fit_transform(clean_train)\n",
    "train_data_features = train_data_features_original.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))\n",
    "dist = np.sum(train_data_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x7fa7c91a18d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lda.LDA(n_topics = 50 , n_iter=300, random_state=1)\n",
    "model.fit(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: گروه گزارش شبک نفر دست خبرگزاری عنوان ماه وارد\n",
      "Topic 1: سفر کشور وزیر ماه خراسان جمهوری امضا سازمان جنوب\n",
      "Topic 2: استان البرز کرج جاده شهرسازی چالوس شورا طالقان امنیت\n",
      "Topic 3: استان بخش توجه کار فراهم ایجاد مجموعه فعالیت انجام\n",
      "Topic 4: کشور اشاره کالا سال رفاه حوزه مقابله رشد مالی\n",
      "Topic 5: سرمایه استان واحد استاندار پروژه تولید گزارش ستاد اشتغال\n",
      "Topic 6: استان نفر ایرنا بیان خبرنگار اعتبار عنوان ساخت آیین\n",
      "Topic 7: تیم خونه فوتبال لیگ فدراسیون بازیکن باشگاه جوان سرمربی\n",
      "Topic 8: رئیس جمهوری شیراز احیا جلسه معاون ایرنا مراکز قرار\n",
      "Topic 9: دانشگاه انقلاب شهدا شهید قم استاد مدافع شهادت دفاع\n",
      "Topic 10: استان ثبت کرمان اسناد گذشته اداره املاک مالکیت مدیرکل\n",
      "Topic 11: حزب وزیر رهبر انتخابات لبنان دولت مجلس تصمیم اعلام\n",
      "Topic 12: کودک شهرستان همکار مساجد برنامه مادر زن کانون توزیع\n",
      "Topic 13: دستگاه خبر اطلاع دادگستری انتشار محل پرونده رسان موضوع\n",
      "Topic 14: شهرستان کیلومتر شامل خرید گندم واقع هکتار ریال دست\n",
      "Topic 15: داعش سوریه عراق گروه نیرو پایگاه عملیات ها خبر\n",
      "Topic 16: مازندران منطقه سرمایه عنوان ادامه چشمه گرفتن گذار جمله\n",
      "Topic 17: جامعه حجت الاسلام حوزه شهرستان جوان دشمن مسئول برگزاری\n",
      "Topic 18: روزنامه گذشته شهروند افراد خصوص جهان وضعیت هشدار هفته\n",
      "Topic 19: none درختان خودرو حمایت هویت تشکیل نیا اهداف نمایندگی\n",
      "Topic 20: امام حسن حضرت اهل بیت مجتبی خدا شب کریم\n",
      "Topic 21: گردشگری میراث دست صنایع اردبیل اداره منابع معاون سرمایه\n",
      "Topic 22: شهر واحد اصفهان مسکن مهر تکمیل بهارستان جمعیت برنامه\n",
      "Topic 23: انگلیس اتحادیه اروپا حضور برگزیت بورس پرس خروج ارزش\n",
      "Topic 24: the and to that in of ایران آمریکا iran\n",
      "Topic 25: بیمار بیمارستان بهداشت کلینیک پزشک درمان دانشگاه دکتر سلامت\n",
      "Topic 26: دولت نظام حقوق دست توجه تاکید مردمی عمل ایجاد\n",
      "Topic 27: های نظام تعداد نیرو حضور آتش یاسوج رئیس ایرنا\n",
      "Topic 28: جمهوری منطقه دیدار حل رئیس مناقشه آذربایجان المللی روسیه\n",
      "Topic 29: استان زندان نشست برنامه ساعت خبر شورا ایرنا شهرکرد\n",
      "Topic 30: فرهنگ ارشاد فیلم وزارت نمایشگاه قرآن آباد وزیر نوش\n",
      "Topic 31: ها اسلام انسان کار نظر مسلمان آن جهان تلاش\n",
      "Topic 32: شهر ایلام خیابان محیط فضا زیست قطع چنار نظر\n",
      "Topic 33: اورژانس اصفهان پایگاه ماموریت استاندارد ها حوادث کلانتر بصورت\n",
      "Topic 34: قانون برنامه سازمان های کاهش فعالیت دانش تاکید نهاد\n",
      "Topic 35: ترامپ سلاح ملل می پلیس شکست کنترل انداز آمریکا\n",
      "Topic 36: بحرین شیخ عیسی قاسم رژیم الله علیه بیانیه اقدام\n",
      "Topic 37: شهردار بنا سازمان شهر توسعه وجود شورا های بخشی\n",
      "Topic 38: کشور بحران نماینده گذشته های مدیر مراسم رییس یافت\n",
      "Topic 39: مواد آب یزد مصرف استان مبارزه تولید درصد اعتیاد\n",
      "Topic 40: ایران دولت طرف کاهش گذشته فصل تهران اجرا افزایش\n",
      "Topic 41: کمک ماه خانواده زمان روش مالی عنوان رمضان خیر\n",
      "Topic 42: افغانستان فروشگاه پاکستان حدود قرار تن تالاب نهبندان نفر\n",
      "Topic 43: ایران چین بیمه آلمان شرکت همکار نشست عضو روابط\n",
      "Topic 44: اقتصاد مقاومت تحقق برنامه بخش تعاون سیاست اهداف تعاونی\n",
      "Topic 45: کشور انقلاب رهبر آمیز نظام گزارش اقدام آغاز بحران\n",
      "Topic 46: بیان اظهار عنوان ادامه اشاره انجام خاطرنشان ایرنا تاکید\n",
      "Topic 47: طرح اجرا آب نیاز قرار زمین اشاره بررسی واحد\n",
      "Topic 48: ورزش حضور بانوان ورزشکار قهرمان حسینی دوشنبه کشف مسابقات\n",
      "Topic 49: آمریکا روسیه قرار مساله ادامه کره مرز رابطه رو\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    np.array(vocab)[np.argsort(topic_dist)]\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models , similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(clean_train_original)\n",
    "corpus = [dictionary.doc2bow(text) for text in clean_train_original]\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000*\"None\" + 0.000*\"اسپانیا\" + -0.000*\"رایانه\" + -0.000*\"ابررایانه\" + -0.000*\"استخر\" + -0.000*\"آمریکا\" + -0.000*\"آلمان\" + -0.000*\"چین\" + -0.000*\"ناتو\" + 0.000*\"وزارت\"\n",
      "----------\n",
      "0.851*\"\" + 0.083*\"اقتصاد\" + 0.069*\"مقاومت\" + 0.068*\"گردشگری\" + 0.067*\"طرح\" + 0.066*\"استان\" + 0.065*\"کشور\" + 0.056*\"شهر\" + 0.055*\"شهرستان\" + 0.053*\"خونه\"\n",
      "----------\n",
      "0.707*\"خونه\" + 0.319*\"تیم\" + 0.311*\"لیگ\" + 0.213*\"مازندران\" + 0.199*\"بازیکن\" + 0.126*\"فوتبال\" + 0.115*\"باشگاه\" + 0.101*\"سرمربی\" + 0.089*\"مرزبان\" + 0.072*\"کریم\"\n",
      "----------\n",
      "-0.488*\"بحرین\" + -0.229*\"عیسی\" + 0.192*\"اقتصاد\" + -0.186*\"شیخ\" + 0.177*\"گردشگری\" + 0.172*\"مقاومت\" + -0.143*\"قاسم\" + 0.111*\"تعاون\" + -0.100*\"حزب\" + 0.099*\"بنا\"\n",
      "----------\n",
      "-0.295*\"اقتصاد\" + 0.290*\"گردشگری\" + -0.274*\"مقاومت\" + 0.241*\"بنا\" + 0.235*\"شیراز\" + -0.202*\"تعاون\" + 0.201*\"احیا\" + 0.198*\"میراث\" + 0.195*\"مرمت\" + 0.158*\"بافت\"\n",
      "----------\n",
      "0.416*\"بحرین\" + 0.248*\"اقتصاد\" + 0.220*\"مقاومت\" + 0.197*\"عیسی\" + 0.167*\"تعاون\" + 0.160*\"شیخ\" + 0.144*\"قاسم\" + 0.143*\"گردشگری\" + 0.110*\"تعاونی\" + -0.101*\"بیمارستان\"\n",
      "----------\n",
      "-0.270*\"اتحادیه\" + -0.248*\"انگلیس\" + -0.224*\"اروپا\" + 0.172*\"شیروان\" + 0.164*\"ع\" + 0.163*\"شهرستان\" + 0.158*\"بحرین\" + 0.156*\"بیمارستان\" + 0.119*\"اهل\" + 0.116*\"امام\"\n",
      "----------\n",
      "-0.251*\"شیروان\" + -0.236*\"بیمارستان\" + 0.227*\"ع\" + 0.171*\"زندان\" + 0.169*\"اهل\" + 0.160*\"بیت\" + 0.148*\"حرم\" + -0.142*\"شهرستان\" + -0.129*\"اعتبار\" + -0.129*\"هاشم\"\n",
      "----------\n",
      "0.412*\"زندان\" + 0.263*\"چهارمحال\" + 0.201*\"بختیار\" + 0.179*\"اتحادیه\" + 0.167*\"انگلیس\" + 0.142*\"اروپا\" + 0.140*\"سرمایه\" + 0.131*\"شهرکرد\" + 0.130*\"بدهی\" + 0.130*\"گردشگری\"\n",
      "----------\n",
      "0.222*\"اتحادیه\" + 0.198*\"انگلیس\" + -0.189*\"اردبیل\" + -0.186*\"زندان\" + 0.178*\"اروپا\" + 0.154*\"شیروان\" + 0.133*\"ع\" + -0.131*\"چهارمحال\" + 0.129*\"بیمارستان\" + -0.121*\"پرونده\"\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(lsi.print_topic(i))\n",
    "    print (\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic 0: 0.090* + 0.004*اقتصاد + 0.004*اورژانس + 0.003*اصفهان + 0.003*مقاومت + 0.003*سال + 0.002*پایگاه + 0.002*ترامپ + 0.002*نظر + 0.002*بیان',\n",
       " 'topic 1: 0.055* + 0.006*the + 0.003*and + 0.003*to + 0.003*that + 0.002*of + 0.002*in + 0.002*قانون + 0.002*ایران + 0.002*آمریکا',\n",
       " 'topic 2: 0.055* + 0.003*بحرین + 0.002*مواد + 0.002*حزب + 0.002*ورزش + 0.002*بیان + 0.002*پیشگیر + 0.002*استان + 0.002*بخش + 0.001*تعاون',\n",
       " 'topic 3: 0.060* + 0.004*سال + 0.003*استان + 0.003*اسناد + 0.002*ثبت + 0.002*کرمان + 0.002*فوتبال + 0.002*طرح + 0.002*گردشگری + 0.002*املاک']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> hdp = models.HdpModel(corpus, dictionary.id2token)\n",
    ">>> hdp.print_topics(topics=4, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"https://d3js.org/d3.v4.min.js\"></script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min'\n",
    "  }\n",
    "});\n",
    "element.append(\"<div id='chart1'></div>\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
